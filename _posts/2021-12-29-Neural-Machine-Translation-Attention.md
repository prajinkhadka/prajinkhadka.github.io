# Neural Machine Translation by jointly learning to align and translate.



| **Color**      | **Meaning** |      | 
| :---        |    :----:   |          ---: |
| Red      |  Notes, Views, Understanding        |    |
| Yellow      | Current paper methods, topics       |    |
| Green      | Reference to other papers       |    |
| Purple      | Doubts, Quesions, Issues       |    |


The paper demonstrates the use of attention with Seq-to-Seq Encoder-Decoder based architecture, training the end to end pipeline for better Machine translation, solving mainly the problems with long range dependency. This was the first paper which used the attention module  with Seq-to-Seq architecture. 


<embed src="https://drive.google.com/viewerng/viewer?embedded=true&url=https://raw.githubusercontent.com/prajinkhadka/annotated-papers/main/NMT_ALIGN_TRANSLATE-compressed.pdf" width="500" height="375">


@misc{title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio},
      year={2016},
      eprint={1409.0473v7},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}